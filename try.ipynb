{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from utils import setup_seed ,  count_parameters  , loading_data ,  ImageDataset\n",
    "from modeling_mae import MAE_ViT \n",
    "from  torch.utils.data import DataLoader \n",
    "from tqdm.auto import tqdm \n",
    "import time \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch.nn.functional as F\n",
    "from configuration import MAEConfig\n",
    "\n",
    "config = MAEConfig()\n",
    "\n",
    "from datasets import load_dataset \n",
    "import torchvision\n",
    "\n",
    "setup_seed(seed=42)\n",
    "\n",
    "COMPILE = False \n",
    "EPOCHS = 100\n",
    "LR = 5e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EVAL_INTERVAL  = 100\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "\n",
    "# Define the transform with resizing to 224x224\n",
    "transform = Compose([\n",
    "    Resize((32, 32)),  # Resizes images to 224x224\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # Normalize with mean and std for 3 channels (RGB)\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset with resizing\n",
    "train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_data)\n",
    "val_dataset = ImageDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader=  DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size =128, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 7.21M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAE_ViT(\n",
       "  (encoder): MAE_Encoder(\n",
       "    (shuffle): PatchShuffle()\n",
       "    (patchify): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (transformer): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): MAE_Decoder(\n",
       "    (transformer): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (head): Linear(in_features=192, out_features=12, bias=True)\n",
       "    (patch2img): Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=2, p2=2, h=16)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modeling_mae import MAE_ViT \n",
    "from configuration import MAEConfig\n",
    "\n",
    "config = MAEConfig()\n",
    "\n",
    "\n",
    "model = MAE_ViT(config=config)\n",
    "print(\"Model Parameters:\",count_parameters(model))\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MAE_ViT:\n\tMissing key(s) in state_dict: \"encoder.cls_token\", \"encoder.pos_embedding\", \"encoder.patchify.weight\", \"encoder.patchify.bias\", \"encoder.transformer.0.norm1.weight\", \"encoder.transformer.0.norm1.bias\", \"encoder.transformer.0.attn.qkv.weight\", \"encoder.transformer.0.attn.proj.weight\", \"encoder.transformer.0.attn.proj.bias\", \"encoder.transformer.0.norm2.weight\", \"encoder.transformer.0.norm2.bias\", \"encoder.transformer.0.mlp.fc1.weight\", \"encoder.transformer.0.mlp.fc1.bias\", \"encoder.transformer.0.mlp.fc2.weight\", \"encoder.transformer.0.mlp.fc2.bias\", \"encoder.transformer.1.norm1.weight\", \"encoder.transformer.1.norm1.bias\", \"encoder.transformer.1.attn.qkv.weight\", \"encoder.transformer.1.attn.proj.weight\", \"encoder.transformer.1.attn.proj.bias\", \"encoder.transformer.1.norm2.weight\", \"encoder.transformer.1.norm2.bias\", \"encoder.transformer.1.mlp.fc1.weight\", \"encoder.transformer.1.mlp.fc1.bias\", \"encoder.transformer.1.mlp.fc2.weight\", \"encoder.transformer.1.mlp.fc2.bias\", \"encoder.transformer.2.norm1.weight\", \"encoder.transformer.2.norm1.bias\", \"encoder.transformer.2.attn.qkv.weight\", \"encoder.transformer.2.attn.proj.weight\", \"encoder.transformer.2.attn.proj.bias\", \"encoder.transformer.2.norm2.weight\", \"encoder.transformer.2.norm2.bias\", \"encoder.transformer.2.mlp.fc1.weight\", \"encoder.transformer.2.mlp.fc1.bias\", \"encoder.transformer.2.mlp.fc2.weight\", \"encoder.transformer.2.mlp.fc2.bias\", \"encoder.transformer.3.norm1.weight\", \"encoder.transformer.3.norm1.bias\", \"encoder.transformer.3.attn.qkv.weight\", \"encoder.transformer.3.attn.proj.weight\", \"encoder.transformer.3.attn.proj.bias\", \"encoder.transformer.3.norm2.weight\", \"encoder.transformer.3.norm2.bias\", \"encoder.transformer.3.mlp.fc1.weight\", \"encoder.transformer.3.mlp.fc1.bias\", \"encoder.transformer.3.mlp.fc2.weight\", \"encoder.transformer.3.mlp.fc2.bias\", \"encoder.transformer.4.norm1.weight\", \"encoder.transformer.4.norm1.bias\", \"encoder.transformer.4.attn.qkv.weight\", \"encoder.transformer.4.attn.proj.weight\", \"encoder.transformer.4.attn.proj.bias\", \"encoder.transformer.4.norm2.weight\", \"encoder.transformer.4.norm2.bias\", \"encoder.transformer.4.mlp.fc1.weight\", \"encoder.transformer.4.mlp.fc1.bias\", \"encoder.transformer.4.mlp.fc2.weight\", \"encoder.transformer.4.mlp.fc2.bias\", \"encoder.transformer.5.norm1.weight\", \"encoder.transformer.5.norm1.bias\", \"encoder.transformer.5.attn.qkv.weight\", \"encoder.transformer.5.attn.proj.weight\", \"encoder.transformer.5.attn.proj.bias\", \"encoder.transformer.5.norm2.weight\", \"encoder.transformer.5.norm2.bias\", \"encoder.transformer.5.mlp.fc1.weight\", \"encoder.transformer.5.mlp.fc1.bias\", \"encoder.transformer.5.mlp.fc2.weight\", \"encoder.transformer.5.mlp.fc2.bias\", \"encoder.transformer.6.norm1.weight\", \"encoder.transformer.6.norm1.bias\", \"encoder.transformer.6.attn.qkv.weight\", \"encoder.transformer.6.attn.proj.weight\", \"encoder.transformer.6.attn.proj.bias\", \"encoder.transformer.6.norm2.weight\", \"encoder.transformer.6.norm2.bias\", \"encoder.transformer.6.mlp.fc1.weight\", \"encoder.transformer.6.mlp.fc1.bias\", \"encoder.transformer.6.mlp.fc2.weight\", \"encoder.transformer.6.mlp.fc2.bias\", \"encoder.transformer.7.norm1.weight\", \"encoder.transformer.7.norm1.bias\", \"encoder.transformer.7.attn.qkv.weight\", \"encoder.transformer.7.attn.proj.weight\", \"encoder.transformer.7.attn.proj.bias\", \"encoder.transformer.7.norm2.weight\", \"encoder.transformer.7.norm2.bias\", \"encoder.transformer.7.mlp.fc1.weight\", \"encoder.transformer.7.mlp.fc1.bias\", \"encoder.transformer.7.mlp.fc2.weight\", \"encoder.transformer.7.mlp.fc2.bias\", \"encoder.transformer.8.norm1.weight\", \"encoder.transformer.8.norm1.bias\", \"encoder.transformer.8.attn.qkv.weight\", \"encoder.transformer.8.attn.proj.weight\", \"encoder.transformer.8.attn.proj.bias\", \"encoder.transformer.8.norm2.weight\", \"encoder.transformer.8.norm2.bias\", \"encoder.transformer.8.mlp.fc1.weight\", \"encoder.transformer.8.mlp.fc1.bias\", \"encoder.transformer.8.mlp.fc2.weight\", \"encoder.transformer.8.mlp.fc2.bias\", \"encoder.transformer.9.norm1.weight\", \"encoder.transformer.9.norm1.bias\", \"encoder.transformer.9.attn.qkv.weight\", \"encoder.transformer.9.attn.proj.weight\", \"encoder.transformer.9.attn.proj.bias\", \"encoder.transformer.9.norm2.weight\", \"encoder.transformer.9.norm2.bias\", \"encoder.transformer.9.mlp.fc1.weight\", \"encoder.transformer.9.mlp.fc1.bias\", \"encoder.transformer.9.mlp.fc2.weight\", \"encoder.transformer.9.mlp.fc2.bias\", \"encoder.transformer.10.norm1.weight\", \"encoder.transformer.10.norm1.bias\", \"encoder.transformer.10.attn.qkv.weight\", \"encoder.transformer.10.attn.proj.weight\", \"encoder.transformer.10.attn.proj.bias\", \"encoder.transformer.10.norm2.weight\", \"encoder.transformer.10.norm2.bias\", \"encoder.transformer.10.mlp.fc1.weight\", \"encoder.transformer.10.mlp.fc1.bias\", \"encoder.transformer.10.mlp.fc2.weight\", \"encoder.transformer.10.mlp.fc2.bias\", \"encoder.transformer.11.norm1.weight\", \"encoder.transformer.11.norm1.bias\", \"encoder.transformer.11.attn.qkv.weight\", \"encoder.transformer.11.attn.proj.weight\", \"encoder.transformer.11.attn.proj.bias\", \"encoder.transformer.11.norm2.weight\", \"encoder.transformer.11.norm2.bias\", \"encoder.transformer.11.mlp.fc1.weight\", \"encoder.transformer.11.mlp.fc1.bias\", \"encoder.transformer.11.mlp.fc2.weight\", \"encoder.transformer.11.mlp.fc2.bias\", \"encoder.layer_norm.weight\", \"encoder.layer_norm.bias\", \"decoder.mask_token\", \"decoder.pos_embedding\", \"decoder.transformer.0.norm1.weight\", \"decoder.transformer.0.norm1.bias\", \"decoder.transformer.0.attn.qkv.weight\", \"decoder.transformer.0.attn.proj.weight\", \"decoder.transformer.0.attn.proj.bias\", \"decoder.transformer.0.norm2.weight\", \"decoder.transformer.0.norm2.bias\", \"decoder.transformer.0.mlp.fc1.weight\", \"decoder.transformer.0.mlp.fc1.bias\", \"decoder.transformer.0.mlp.fc2.weight\", \"decoder.transformer.0.mlp.fc2.bias\", \"decoder.transformer.1.norm1.weight\", \"decoder.transformer.1.norm1.bias\", \"decoder.transformer.1.attn.qkv.weight\", \"decoder.transformer.1.attn.proj.weight\", \"decoder.transformer.1.attn.proj.bias\", \"decoder.transformer.1.norm2.weight\", \"decoder.transformer.1.norm2.bias\", \"decoder.transformer.1.mlp.fc1.weight\", \"decoder.transformer.1.mlp.fc1.bias\", \"decoder.transformer.1.mlp.fc2.weight\", \"decoder.transformer.1.mlp.fc2.bias\", \"decoder.transformer.2.norm1.weight\", \"decoder.transformer.2.norm1.bias\", \"decoder.transformer.2.attn.qkv.weight\", \"decoder.transformer.2.attn.proj.weight\", \"decoder.transformer.2.attn.proj.bias\", \"decoder.transformer.2.norm2.weight\", \"decoder.transformer.2.norm2.bias\", \"decoder.transformer.2.mlp.fc1.weight\", \"decoder.transformer.2.mlp.fc1.bias\", \"decoder.transformer.2.mlp.fc2.weight\", \"decoder.transformer.2.mlp.fc2.bias\", \"decoder.transformer.3.norm1.weight\", \"decoder.transformer.3.norm1.bias\", \"decoder.transformer.3.attn.qkv.weight\", \"decoder.transformer.3.attn.proj.weight\", \"decoder.transformer.3.attn.proj.bias\", \"decoder.transformer.3.norm2.weight\", \"decoder.transformer.3.norm2.bias\", \"decoder.transformer.3.mlp.fc1.weight\", \"decoder.transformer.3.mlp.fc1.bias\", \"decoder.transformer.3.mlp.fc2.weight\", \"decoder.transformer.3.mlp.fc2.bias\", \"decoder.head.weight\", \"decoder.head.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.encoder.cls_token\", \"_orig_mod.encoder.pos_embedding\", \"_orig_mod.encoder.patchify.weight\", \"_orig_mod.encoder.patchify.bias\", \"_orig_mod.encoder.transformer.0.norm1.weight\", \"_orig_mod.encoder.transformer.0.norm1.bias\", \"_orig_mod.encoder.transformer.0.attn.qkv.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.bias\", \"_orig_mod.encoder.transformer.0.norm2.weight\", \"_orig_mod.encoder.transformer.0.norm2.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.1.norm1.weight\", \"_orig_mod.encoder.transformer.1.norm1.bias\", \"_orig_mod.encoder.transformer.1.attn.qkv.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.bias\", \"_orig_mod.encoder.transformer.1.norm2.weight\", \"_orig_mod.encoder.transformer.1.norm2.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.2.norm1.weight\", \"_orig_mod.encoder.transformer.2.norm1.bias\", \"_orig_mod.encoder.transformer.2.attn.qkv.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.bias\", \"_orig_mod.encoder.transformer.2.norm2.weight\", \"_orig_mod.encoder.transformer.2.norm2.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.3.norm1.weight\", \"_orig_mod.encoder.transformer.3.norm1.bias\", \"_orig_mod.encoder.transformer.3.attn.qkv.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.bias\", \"_orig_mod.encoder.transformer.3.norm2.weight\", \"_orig_mod.encoder.transformer.3.norm2.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.4.norm1.weight\", \"_orig_mod.encoder.transformer.4.norm1.bias\", \"_orig_mod.encoder.transformer.4.attn.qkv.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.bias\", \"_orig_mod.encoder.transformer.4.norm2.weight\", \"_orig_mod.encoder.transformer.4.norm2.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.5.norm1.weight\", \"_orig_mod.encoder.transformer.5.norm1.bias\", \"_orig_mod.encoder.transformer.5.attn.qkv.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.bias\", \"_orig_mod.encoder.transformer.5.norm2.weight\", \"_orig_mod.encoder.transformer.5.norm2.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.6.norm1.weight\", \"_orig_mod.encoder.transformer.6.norm1.bias\", \"_orig_mod.encoder.transformer.6.attn.qkv.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.bias\", \"_orig_mod.encoder.transformer.6.norm2.weight\", \"_orig_mod.encoder.transformer.6.norm2.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.7.norm1.weight\", \"_orig_mod.encoder.transformer.7.norm1.bias\", \"_orig_mod.encoder.transformer.7.attn.qkv.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.bias\", \"_orig_mod.encoder.transformer.7.norm2.weight\", \"_orig_mod.encoder.transformer.7.norm2.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.8.norm1.weight\", \"_orig_mod.encoder.transformer.8.norm1.bias\", \"_orig_mod.encoder.transformer.8.attn.qkv.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.bias\", \"_orig_mod.encoder.transformer.8.norm2.weight\", \"_orig_mod.encoder.transformer.8.norm2.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.9.norm1.weight\", \"_orig_mod.encoder.transformer.9.norm1.bias\", \"_orig_mod.encoder.transformer.9.attn.qkv.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.bias\", \"_orig_mod.encoder.transformer.9.norm2.weight\", \"_orig_mod.encoder.transformer.9.norm2.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.10.norm1.weight\", \"_orig_mod.encoder.transformer.10.norm1.bias\", \"_orig_mod.encoder.transformer.10.attn.qkv.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.bias\", \"_orig_mod.encoder.transformer.10.norm2.weight\", \"_orig_mod.encoder.transformer.10.norm2.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.11.norm1.weight\", \"_orig_mod.encoder.transformer.11.norm1.bias\", \"_orig_mod.encoder.transformer.11.attn.qkv.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.bias\", \"_orig_mod.encoder.transformer.11.norm2.weight\", \"_orig_mod.encoder.transformer.11.norm2.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc2.bias\", \"_orig_mod.encoder.layer_norm.weight\", \"_orig_mod.encoder.layer_norm.bias\", \"_orig_mod.decoder.mask_token\", \"_orig_mod.decoder.pos_embedding\", \"_orig_mod.decoder.transformer.0.norm1.weight\", \"_orig_mod.decoder.transformer.0.norm1.bias\", \"_orig_mod.decoder.transformer.0.attn.qkv.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.bias\", \"_orig_mod.decoder.transformer.0.norm2.weight\", \"_orig_mod.decoder.transformer.0.norm2.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.1.norm1.weight\", \"_orig_mod.decoder.transformer.1.norm1.bias\", \"_orig_mod.decoder.transformer.1.attn.qkv.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.bias\", \"_orig_mod.decoder.transformer.1.norm2.weight\", \"_orig_mod.decoder.transformer.1.norm2.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.2.norm1.weight\", \"_orig_mod.decoder.transformer.2.norm1.bias\", \"_orig_mod.decoder.transformer.2.attn.qkv.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.bias\", \"_orig_mod.decoder.transformer.2.norm2.weight\", \"_orig_mod.decoder.transformer.2.norm2.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.3.norm1.weight\", \"_orig_mod.decoder.transformer.3.norm1.bias\", \"_orig_mod.decoder.transformer.3.attn.qkv.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.bias\", \"_orig_mod.decoder.transformer.3.norm2.weight\", \"_orig_mod.decoder.transformer.3.norm2.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.decoder.head.weight\", \"_orig_mod.decoder.head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/teamspace/studios/this_studio/real_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MAE_ViT:\n\tMissing key(s) in state_dict: \"encoder.cls_token\", \"encoder.pos_embedding\", \"encoder.patchify.weight\", \"encoder.patchify.bias\", \"encoder.transformer.0.norm1.weight\", \"encoder.transformer.0.norm1.bias\", \"encoder.transformer.0.attn.qkv.weight\", \"encoder.transformer.0.attn.proj.weight\", \"encoder.transformer.0.attn.proj.bias\", \"encoder.transformer.0.norm2.weight\", \"encoder.transformer.0.norm2.bias\", \"encoder.transformer.0.mlp.fc1.weight\", \"encoder.transformer.0.mlp.fc1.bias\", \"encoder.transformer.0.mlp.fc2.weight\", \"encoder.transformer.0.mlp.fc2.bias\", \"encoder.transformer.1.norm1.weight\", \"encoder.transformer.1.norm1.bias\", \"encoder.transformer.1.attn.qkv.weight\", \"encoder.transformer.1.attn.proj.weight\", \"encoder.transformer.1.attn.proj.bias\", \"encoder.transformer.1.norm2.weight\", \"encoder.transformer.1.norm2.bias\", \"encoder.transformer.1.mlp.fc1.weight\", \"encoder.transformer.1.mlp.fc1.bias\", \"encoder.transformer.1.mlp.fc2.weight\", \"encoder.transformer.1.mlp.fc2.bias\", \"encoder.transformer.2.norm1.weight\", \"encoder.transformer.2.norm1.bias\", \"encoder.transformer.2.attn.qkv.weight\", \"encoder.transformer.2.attn.proj.weight\", \"encoder.transformer.2.attn.proj.bias\", \"encoder.transformer.2.norm2.weight\", \"encoder.transformer.2.norm2.bias\", \"encoder.transformer.2.mlp.fc1.weight\", \"encoder.transformer.2.mlp.fc1.bias\", \"encoder.transformer.2.mlp.fc2.weight\", \"encoder.transformer.2.mlp.fc2.bias\", \"encoder.transformer.3.norm1.weight\", \"encoder.transformer.3.norm1.bias\", \"encoder.transformer.3.attn.qkv.weight\", \"encoder.transformer.3.attn.proj.weight\", \"encoder.transformer.3.attn.proj.bias\", \"encoder.transformer.3.norm2.weight\", \"encoder.transformer.3.norm2.bias\", \"encoder.transformer.3.mlp.fc1.weight\", \"encoder.transformer.3.mlp.fc1.bias\", \"encoder.transformer.3.mlp.fc2.weight\", \"encoder.transformer.3.mlp.fc2.bias\", \"encoder.transformer.4.norm1.weight\", \"encoder.transformer.4.norm1.bias\", \"encoder.transformer.4.attn.qkv.weight\", \"encoder.transformer.4.attn.proj.weight\", \"encoder.transformer.4.attn.proj.bias\", \"encoder.transformer.4.norm2.weight\", \"encoder.transformer.4.norm2.bias\", \"encoder.transformer.4.mlp.fc1.weight\", \"encoder.transformer.4.mlp.fc1.bias\", \"encoder.transformer.4.mlp.fc2.weight\", \"encoder.transformer.4.mlp.fc2.bias\", \"encoder.transformer.5.norm1.weight\", \"encoder.transformer.5.norm1.bias\", \"encoder.transformer.5.attn.qkv.weight\", \"encoder.transformer.5.attn.proj.weight\", \"encoder.transformer.5.attn.proj.bias\", \"encoder.transformer.5.norm2.weight\", \"encoder.transformer.5.norm2.bias\", \"encoder.transformer.5.mlp.fc1.weight\", \"encoder.transformer.5.mlp.fc1.bias\", \"encoder.transformer.5.mlp.fc2.weight\", \"encoder.transformer.5.mlp.fc2.bias\", \"encoder.transformer.6.norm1.weight\", \"encoder.transformer.6.norm1.bias\", \"encoder.transformer.6.attn.qkv.weight\", \"encoder.transformer.6.attn.proj.weight\", \"encoder.transformer.6.attn.proj.bias\", \"encoder.transformer.6.norm2.weight\", \"encoder.transformer.6.norm2.bias\", \"encoder.transformer.6.mlp.fc1.weight\", \"encoder.transformer.6.mlp.fc1.bias\", \"encoder.transformer.6.mlp.fc2.weight\", \"encoder.transformer.6.mlp.fc2.bias\", \"encoder.transformer.7.norm1.weight\", \"encoder.transformer.7.norm1.bias\", \"encoder.transformer.7.attn.qkv.weight\", \"encoder.transformer.7.attn.proj.weight\", \"encoder.transformer.7.attn.proj.bias\", \"encoder.transformer.7.norm2.weight\", \"encoder.transformer.7.norm2.bias\", \"encoder.transformer.7.mlp.fc1.weight\", \"encoder.transformer.7.mlp.fc1.bias\", \"encoder.transformer.7.mlp.fc2.weight\", \"encoder.transformer.7.mlp.fc2.bias\", \"encoder.transformer.8.norm1.weight\", \"encoder.transformer.8.norm1.bias\", \"encoder.transformer.8.attn.qkv.weight\", \"encoder.transformer.8.attn.proj.weight\", \"encoder.transformer.8.attn.proj.bias\", \"encoder.transformer.8.norm2.weight\", \"encoder.transformer.8.norm2.bias\", \"encoder.transformer.8.mlp.fc1.weight\", \"encoder.transformer.8.mlp.fc1.bias\", \"encoder.transformer.8.mlp.fc2.weight\", \"encoder.transformer.8.mlp.fc2.bias\", \"encoder.transformer.9.norm1.weight\", \"encoder.transformer.9.norm1.bias\", \"encoder.transformer.9.attn.qkv.weight\", \"encoder.transformer.9.attn.proj.weight\", \"encoder.transformer.9.attn.proj.bias\", \"encoder.transformer.9.norm2.weight\", \"encoder.transformer.9.norm2.bias\", \"encoder.transformer.9.mlp.fc1.weight\", \"encoder.transformer.9.mlp.fc1.bias\", \"encoder.transformer.9.mlp.fc2.weight\", \"encoder.transformer.9.mlp.fc2.bias\", \"encoder.transformer.10.norm1.weight\", \"encoder.transformer.10.norm1.bias\", \"encoder.transformer.10.attn.qkv.weight\", \"encoder.transformer.10.attn.proj.weight\", \"encoder.transformer.10.attn.proj.bias\", \"encoder.transformer.10.norm2.weight\", \"encoder.transformer.10.norm2.bias\", \"encoder.transformer.10.mlp.fc1.weight\", \"encoder.transformer.10.mlp.fc1.bias\", \"encoder.transformer.10.mlp.fc2.weight\", \"encoder.transformer.10.mlp.fc2.bias\", \"encoder.transformer.11.norm1.weight\", \"encoder.transformer.11.norm1.bias\", \"encoder.transformer.11.attn.qkv.weight\", \"encoder.transformer.11.attn.proj.weight\", \"encoder.transformer.11.attn.proj.bias\", \"encoder.transformer.11.norm2.weight\", \"encoder.transformer.11.norm2.bias\", \"encoder.transformer.11.mlp.fc1.weight\", \"encoder.transformer.11.mlp.fc1.bias\", \"encoder.transformer.11.mlp.fc2.weight\", \"encoder.transformer.11.mlp.fc2.bias\", \"encoder.layer_norm.weight\", \"encoder.layer_norm.bias\", \"decoder.mask_token\", \"decoder.pos_embedding\", \"decoder.transformer.0.norm1.weight\", \"decoder.transformer.0.norm1.bias\", \"decoder.transformer.0.attn.qkv.weight\", \"decoder.transformer.0.attn.proj.weight\", \"decoder.transformer.0.attn.proj.bias\", \"decoder.transformer.0.norm2.weight\", \"decoder.transformer.0.norm2.bias\", \"decoder.transformer.0.mlp.fc1.weight\", \"decoder.transformer.0.mlp.fc1.bias\", \"decoder.transformer.0.mlp.fc2.weight\", \"decoder.transformer.0.mlp.fc2.bias\", \"decoder.transformer.1.norm1.weight\", \"decoder.transformer.1.norm1.bias\", \"decoder.transformer.1.attn.qkv.weight\", \"decoder.transformer.1.attn.proj.weight\", \"decoder.transformer.1.attn.proj.bias\", \"decoder.transformer.1.norm2.weight\", \"decoder.transformer.1.norm2.bias\", \"decoder.transformer.1.mlp.fc1.weight\", \"decoder.transformer.1.mlp.fc1.bias\", \"decoder.transformer.1.mlp.fc2.weight\", \"decoder.transformer.1.mlp.fc2.bias\", \"decoder.transformer.2.norm1.weight\", \"decoder.transformer.2.norm1.bias\", \"decoder.transformer.2.attn.qkv.weight\", \"decoder.transformer.2.attn.proj.weight\", \"decoder.transformer.2.attn.proj.bias\", \"decoder.transformer.2.norm2.weight\", \"decoder.transformer.2.norm2.bias\", \"decoder.transformer.2.mlp.fc1.weight\", \"decoder.transformer.2.mlp.fc1.bias\", \"decoder.transformer.2.mlp.fc2.weight\", \"decoder.transformer.2.mlp.fc2.bias\", \"decoder.transformer.3.norm1.weight\", \"decoder.transformer.3.norm1.bias\", \"decoder.transformer.3.attn.qkv.weight\", \"decoder.transformer.3.attn.proj.weight\", \"decoder.transformer.3.attn.proj.bias\", \"decoder.transformer.3.norm2.weight\", \"decoder.transformer.3.norm2.bias\", \"decoder.transformer.3.mlp.fc1.weight\", \"decoder.transformer.3.mlp.fc1.bias\", \"decoder.transformer.3.mlp.fc2.weight\", \"decoder.transformer.3.mlp.fc2.bias\", \"decoder.head.weight\", \"decoder.head.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.encoder.cls_token\", \"_orig_mod.encoder.pos_embedding\", \"_orig_mod.encoder.patchify.weight\", \"_orig_mod.encoder.patchify.bias\", \"_orig_mod.encoder.transformer.0.norm1.weight\", \"_orig_mod.encoder.transformer.0.norm1.bias\", \"_orig_mod.encoder.transformer.0.attn.qkv.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.bias\", \"_orig_mod.encoder.transformer.0.norm2.weight\", \"_orig_mod.encoder.transformer.0.norm2.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.1.norm1.weight\", \"_orig_mod.encoder.transformer.1.norm1.bias\", \"_orig_mod.encoder.transformer.1.attn.qkv.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.bias\", \"_orig_mod.encoder.transformer.1.norm2.weight\", \"_orig_mod.encoder.transformer.1.norm2.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.2.norm1.weight\", \"_orig_mod.encoder.transformer.2.norm1.bias\", \"_orig_mod.encoder.transformer.2.attn.qkv.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.bias\", \"_orig_mod.encoder.transformer.2.norm2.weight\", \"_orig_mod.encoder.transformer.2.norm2.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.3.norm1.weight\", \"_orig_mod.encoder.transformer.3.norm1.bias\", \"_orig_mod.encoder.transformer.3.attn.qkv.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.bias\", \"_orig_mod.encoder.transformer.3.norm2.weight\", \"_orig_mod.encoder.transformer.3.norm2.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.4.norm1.weight\", \"_orig_mod.encoder.transformer.4.norm1.bias\", \"_orig_mod.encoder.transformer.4.attn.qkv.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.bias\", \"_orig_mod.encoder.transformer.4.norm2.weight\", \"_orig_mod.encoder.transformer.4.norm2.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.5.norm1.weight\", \"_orig_mod.encoder.transformer.5.norm1.bias\", \"_orig_mod.encoder.transformer.5.attn.qkv.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.bias\", \"_orig_mod.encoder.transformer.5.norm2.weight\", \"_orig_mod.encoder.transformer.5.norm2.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.6.norm1.weight\", \"_orig_mod.encoder.transformer.6.norm1.bias\", \"_orig_mod.encoder.transformer.6.attn.qkv.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.bias\", \"_orig_mod.encoder.transformer.6.norm2.weight\", \"_orig_mod.encoder.transformer.6.norm2.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.7.norm1.weight\", \"_orig_mod.encoder.transformer.7.norm1.bias\", \"_orig_mod.encoder.transformer.7.attn.qkv.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.bias\", \"_orig_mod.encoder.transformer.7.norm2.weight\", \"_orig_mod.encoder.transformer.7.norm2.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.8.norm1.weight\", \"_orig_mod.encoder.transformer.8.norm1.bias\", \"_orig_mod.encoder.transformer.8.attn.qkv.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.bias\", \"_orig_mod.encoder.transformer.8.norm2.weight\", \"_orig_mod.encoder.transformer.8.norm2.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.9.norm1.weight\", \"_orig_mod.encoder.transformer.9.norm1.bias\", \"_orig_mod.encoder.transformer.9.attn.qkv.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.bias\", \"_orig_mod.encoder.transformer.9.norm2.weight\", \"_orig_mod.encoder.transformer.9.norm2.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.10.norm1.weight\", \"_orig_mod.encoder.transformer.10.norm1.bias\", \"_orig_mod.encoder.transformer.10.attn.qkv.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.bias\", \"_orig_mod.encoder.transformer.10.norm2.weight\", \"_orig_mod.encoder.transformer.10.norm2.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.11.norm1.weight\", \"_orig_mod.encoder.transformer.11.norm1.bias\", \"_orig_mod.encoder.transformer.11.attn.qkv.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.bias\", \"_orig_mod.encoder.transformer.11.norm2.weight\", \"_orig_mod.encoder.transformer.11.norm2.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc2.bias\", \"_orig_mod.encoder.layer_norm.weight\", \"_orig_mod.encoder.layer_norm.bias\", \"_orig_mod.decoder.mask_token\", \"_orig_mod.decoder.pos_embedding\", \"_orig_mod.decoder.transformer.0.norm1.weight\", \"_orig_mod.decoder.transformer.0.norm1.bias\", \"_orig_mod.decoder.transformer.0.attn.qkv.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.bias\", \"_orig_mod.decoder.transformer.0.norm2.weight\", \"_orig_mod.decoder.transformer.0.norm2.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.1.norm1.weight\", \"_orig_mod.decoder.transformer.1.norm1.bias\", \"_orig_mod.decoder.transformer.1.attn.qkv.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.bias\", \"_orig_mod.decoder.transformer.1.norm2.weight\", \"_orig_mod.decoder.transformer.1.norm2.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.2.norm1.weight\", \"_orig_mod.decoder.transformer.2.norm1.bias\", \"_orig_mod.decoder.transformer.2.attn.qkv.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.bias\", \"_orig_mod.decoder.transformer.2.norm2.weight\", \"_orig_mod.decoder.transformer.2.norm2.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.3.norm1.weight\", \"_orig_mod.decoder.transformer.3.norm1.bias\", \"_orig_mod.decoder.transformer.3.attn.qkv.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.bias\", \"_orig_mod.decoder.transformer.3.norm2.weight\", \"_orig_mod.decoder.transformer.3.norm2.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.decoder.head.weight\", \"_orig_mod.decoder.head.bias\". "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/teamspace/studios/this_studio/real_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = 16\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "def unpatchify(x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = 16\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "imagenet_mean = np.array([0.5, 0.5, 0.56])\n",
    "imagenet_std = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "def show_image(image, title='', mean=imagenet_mean, std=imagenet_std):\n",
    "    \"\"\"\n",
    "    Displays an image after denormalizing and clipping the values.\n",
    "    \n",
    "    Args:\n",
    "        image (Tensor): Image tensor in NHWC format.\n",
    "        title (str): Title of the image.\n",
    "        mean (np.array): Mean used for normalization.\n",
    "        std (np.array): Standard deviation used for normalization.\n",
    "    \"\"\"\n",
    "    # image is expected to be [H, W, 3] after permuting\n",
    "    assert image.shape[2] == 3\n",
    "    image = torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int()\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def run_inference(image, model, input_size=(32, 32), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    Runs inference using the given model and displays various stages of the image.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): Input image.\n",
    "        model (torch.nn.Module): Pre-trained model.\n",
    "        input_size (tuple): Size to resize the image to.\n",
    "        mean (tuple): Mean for normalization.\n",
    "        std (tuple): Standard deviation for normalization.\n",
    "    \"\"\"\n",
    "    # Preprocessing: resize, to tensor, normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),  # Resize to the model's input size\n",
    "        transforms.ToTensor(),  # Convert image to Tensor\n",
    "        transforms.Normalize(mean=mean, std=std)  # Normalize using given mean and std\n",
    "    ])\n",
    "\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    image = image.convert('RGB')  # Ensure the image is in RGB format\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Transform and add batch dimension\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    image_tensor = image_tensor.to(\"cpu\")  # Move image to CPU if necessary\n",
    "\n",
    "    # No gradient needed during inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        out, mask = model(image_tensor)\n",
    "\n",
    "        # Move outputs to CPU and detach\n",
    "        y = out.detach().cpu()\n",
    "        mask = mask.detach().cpu()\n",
    "\n",
    "        # Convert tensors from NCHW to NHWC format for visualization\n",
    "        x = image_tensor.permute(0, 2, 3, 1).cpu()  # Original input image\n",
    "        y = y.permute(0, 2, 3, 1)  # Reconstructed output\n",
    "        mask = mask.permute(0, 2, 3, 1)  # Mask output\n",
    "\n",
    "        # Generate masked image (original image with mask applied)\n",
    "        im_masked = x * (1 - mask)\n",
    "\n",
    "        # Combine masked and reconstructed patches for the final output\n",
    "        im_paste = im_masked + y * mask\n",
    "\n",
    "        # Display results in a 1x4 grid\n",
    "        plt.rcParams['figure.figsize'] = [12, 12]\n",
    "\n",
    "        plt.subplot(1, 4, 1)\n",
    "        show_image(x[0], \"Original\", mean, std)\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        show_image(im_masked[0], \"Masked\", mean, std)\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        show_image(y[0], \"Reconstruction\", mean, std)\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        show_image(im_paste[0], \"Reconstruction + Visible\", mean, std)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'\n",
    "image = Image.open(image_path)\n",
    "# Ensure you have the correct model initialized\n",
    "# model = MAE_ViT(...)  # Use your actual model initialization here\n",
    "run_inference(image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 120\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "WEIGHT_DECAY= 1e-4\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_func = lambda epoch: min((epoch + 1) / (10 + 1e-8), 0.5 * (math.cos(epoch / EPOCHS * math.pi) + 1))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_func, verbose=True)\n",
    "\n",
    "print(\"Starting to train\")\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "step_times = []\n",
    "grad_norms = []\n",
    "\n",
    "model = torch.compile(model)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def calculate_loss(preds , image , mask,mask_ratio):\n",
    "    return torch.mean((preds - image) ** 2 * mask) / mask_ratio\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    step_times = []\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for step, (image, _) in progress_bar:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.to(DEVICE)\n",
    "\n",
    "        image = image.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out, mask = model(image)\n",
    "        loss = calculate_loss(preds=out, image=image, mask=mask, mask_ratio=0.75)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_losses.append(loss.item())\n",
    "        step_times.append((time.time() - start_time) * 1000)  # Step time in milliseconds\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'step_time': f\"{(time.time() - start_time) * 1000:.4F}ms\",\n",
    "            'lr': f\"{lr_scheduler.get_last_lr()[0]:.6f}\"\n",
    "        })\n",
    "\n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    epoch_val_losses = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Validation {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        for val_step, (val_image, _) in val_progress_bar:\n",
    "            val_image = val_image.to(DEVICE)\n",
    "            \n",
    "            out, mask = model(val_image)\n",
    "            val_loss = calculate_loss(preds=out, image=val_image, mask=mask, mask_ratio=0.75)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"real_model.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'\n",
    "img = Image.open(image_path)\n",
    "run_inference(img, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Validation Loss')\n",
    "plt.xlabel('Evaluation Interval')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Losses Over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Evaluation Interval')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Losses Over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define paths\n",
    "model_path = 'new_model_2.pth'\n",
    "optimizer_path = 'optimizer.pth'\n",
    "scheduler_path = 'scheduler.pth'\n",
    "\n",
    "# Save model, optimizer, and scheduler\n",
    "def save_checkpoint(model, optimizer, scheduler, model_path, optimizer_path, scheduler_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, model_path)\n",
    "\n",
    "# Example usage\n",
    "save_checkpoint(model, optimizer, scheduler, model_path, optimizer_path, scheduler_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 13\u001b[0m model  \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(model, model_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(model,model_path):\n\u001b[1;32m      8\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m----> 9\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define paths\n",
    "model_path = '/teamspace/studios/this_studio/real_model.pt'\n",
    "\n",
    "# Load model, optimizer, and scheduler\n",
    "def load_checkpoint(model,model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model  = load_checkpoint(model,model_path)\n",
    "model.eval()\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[6], line 13\n",
    "      9     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "     11     return model\n",
    "---> 13 model  = load_checkpoint(model,model_path)\n",
    "     14 model.eval()\n",
    "\n",
    "Cell In[6], line 9\n",
    "      7 def load_checkpoint(model,model_path):\n",
    "      8     checkpoint = torch.load(model_path)\n",
    "----> 9     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "     11     return model\n",
    "\n",
    "KeyError: 'model_state_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MAE_ViT:\n\tMissing key(s) in state_dict: \"encoder.cls_token\", \"encoder.pos_embedding\", \"encoder.patchify.weight\", \"encoder.patchify.bias\", \"encoder.transformer.0.norm1.weight\", \"encoder.transformer.0.norm1.bias\", \"encoder.transformer.0.attn.qkv.weight\", \"encoder.transformer.0.attn.proj.weight\", \"encoder.transformer.0.attn.proj.bias\", \"encoder.transformer.0.norm2.weight\", \"encoder.transformer.0.norm2.bias\", \"encoder.transformer.0.mlp.fc1.weight\", \"encoder.transformer.0.mlp.fc1.bias\", \"encoder.transformer.0.mlp.fc2.weight\", \"encoder.transformer.0.mlp.fc2.bias\", \"encoder.transformer.1.norm1.weight\", \"encoder.transformer.1.norm1.bias\", \"encoder.transformer.1.attn.qkv.weight\", \"encoder.transformer.1.attn.proj.weight\", \"encoder.transformer.1.attn.proj.bias\", \"encoder.transformer.1.norm2.weight\", \"encoder.transformer.1.norm2.bias\", \"encoder.transformer.1.mlp.fc1.weight\", \"encoder.transformer.1.mlp.fc1.bias\", \"encoder.transformer.1.mlp.fc2.weight\", \"encoder.transformer.1.mlp.fc2.bias\", \"encoder.transformer.2.norm1.weight\", \"encoder.transformer.2.norm1.bias\", \"encoder.transformer.2.attn.qkv.weight\", \"encoder.transformer.2.attn.proj.weight\", \"encoder.transformer.2.attn.proj.bias\", \"encoder.transformer.2.norm2.weight\", \"encoder.transformer.2.norm2.bias\", \"encoder.transformer.2.mlp.fc1.weight\", \"encoder.transformer.2.mlp.fc1.bias\", \"encoder.transformer.2.mlp.fc2.weight\", \"encoder.transformer.2.mlp.fc2.bias\", \"encoder.transformer.3.norm1.weight\", \"encoder.transformer.3.norm1.bias\", \"encoder.transformer.3.attn.qkv.weight\", \"encoder.transformer.3.attn.proj.weight\", \"encoder.transformer.3.attn.proj.bias\", \"encoder.transformer.3.norm2.weight\", \"encoder.transformer.3.norm2.bias\", \"encoder.transformer.3.mlp.fc1.weight\", \"encoder.transformer.3.mlp.fc1.bias\", \"encoder.transformer.3.mlp.fc2.weight\", \"encoder.transformer.3.mlp.fc2.bias\", \"encoder.transformer.4.norm1.weight\", \"encoder.transformer.4.norm1.bias\", \"encoder.transformer.4.attn.qkv.weight\", \"encoder.transformer.4.attn.proj.weight\", \"encoder.transformer.4.attn.proj.bias\", \"encoder.transformer.4.norm2.weight\", \"encoder.transformer.4.norm2.bias\", \"encoder.transformer.4.mlp.fc1.weight\", \"encoder.transformer.4.mlp.fc1.bias\", \"encoder.transformer.4.mlp.fc2.weight\", \"encoder.transformer.4.mlp.fc2.bias\", \"encoder.transformer.5.norm1.weight\", \"encoder.transformer.5.norm1.bias\", \"encoder.transformer.5.attn.qkv.weight\", \"encoder.transformer.5.attn.proj.weight\", \"encoder.transformer.5.attn.proj.bias\", \"encoder.transformer.5.norm2.weight\", \"encoder.transformer.5.norm2.bias\", \"encoder.transformer.5.mlp.fc1.weight\", \"encoder.transformer.5.mlp.fc1.bias\", \"encoder.transformer.5.mlp.fc2.weight\", \"encoder.transformer.5.mlp.fc2.bias\", \"encoder.transformer.6.norm1.weight\", \"encoder.transformer.6.norm1.bias\", \"encoder.transformer.6.attn.qkv.weight\", \"encoder.transformer.6.attn.proj.weight\", \"encoder.transformer.6.attn.proj.bias\", \"encoder.transformer.6.norm2.weight\", \"encoder.transformer.6.norm2.bias\", \"encoder.transformer.6.mlp.fc1.weight\", \"encoder.transformer.6.mlp.fc1.bias\", \"encoder.transformer.6.mlp.fc2.weight\", \"encoder.transformer.6.mlp.fc2.bias\", \"encoder.transformer.7.norm1.weight\", \"encoder.transformer.7.norm1.bias\", \"encoder.transformer.7.attn.qkv.weight\", \"encoder.transformer.7.attn.proj.weight\", \"encoder.transformer.7.attn.proj.bias\", \"encoder.transformer.7.norm2.weight\", \"encoder.transformer.7.norm2.bias\", \"encoder.transformer.7.mlp.fc1.weight\", \"encoder.transformer.7.mlp.fc1.bias\", \"encoder.transformer.7.mlp.fc2.weight\", \"encoder.transformer.7.mlp.fc2.bias\", \"encoder.transformer.8.norm1.weight\", \"encoder.transformer.8.norm1.bias\", \"encoder.transformer.8.attn.qkv.weight\", \"encoder.transformer.8.attn.proj.weight\", \"encoder.transformer.8.attn.proj.bias\", \"encoder.transformer.8.norm2.weight\", \"encoder.transformer.8.norm2.bias\", \"encoder.transformer.8.mlp.fc1.weight\", \"encoder.transformer.8.mlp.fc1.bias\", \"encoder.transformer.8.mlp.fc2.weight\", \"encoder.transformer.8.mlp.fc2.bias\", \"encoder.transformer.9.norm1.weight\", \"encoder.transformer.9.norm1.bias\", \"encoder.transformer.9.attn.qkv.weight\", \"encoder.transformer.9.attn.proj.weight\", \"encoder.transformer.9.attn.proj.bias\", \"encoder.transformer.9.norm2.weight\", \"encoder.transformer.9.norm2.bias\", \"encoder.transformer.9.mlp.fc1.weight\", \"encoder.transformer.9.mlp.fc1.bias\", \"encoder.transformer.9.mlp.fc2.weight\", \"encoder.transformer.9.mlp.fc2.bias\", \"encoder.transformer.10.norm1.weight\", \"encoder.transformer.10.norm1.bias\", \"encoder.transformer.10.attn.qkv.weight\", \"encoder.transformer.10.attn.proj.weight\", \"encoder.transformer.10.attn.proj.bias\", \"encoder.transformer.10.norm2.weight\", \"encoder.transformer.10.norm2.bias\", \"encoder.transformer.10.mlp.fc1.weight\", \"encoder.transformer.10.mlp.fc1.bias\", \"encoder.transformer.10.mlp.fc2.weight\", \"encoder.transformer.10.mlp.fc2.bias\", \"encoder.transformer.11.norm1.weight\", \"encoder.transformer.11.norm1.bias\", \"encoder.transformer.11.attn.qkv.weight\", \"encoder.transformer.11.attn.proj.weight\", \"encoder.transformer.11.attn.proj.bias\", \"encoder.transformer.11.norm2.weight\", \"encoder.transformer.11.norm2.bias\", \"encoder.transformer.11.mlp.fc1.weight\", \"encoder.transformer.11.mlp.fc1.bias\", \"encoder.transformer.11.mlp.fc2.weight\", \"encoder.transformer.11.mlp.fc2.bias\", \"encoder.layer_norm.weight\", \"encoder.layer_norm.bias\", \"decoder.mask_token\", \"decoder.pos_embedding\", \"decoder.transformer.0.norm1.weight\", \"decoder.transformer.0.norm1.bias\", \"decoder.transformer.0.attn.qkv.weight\", \"decoder.transformer.0.attn.proj.weight\", \"decoder.transformer.0.attn.proj.bias\", \"decoder.transformer.0.norm2.weight\", \"decoder.transformer.0.norm2.bias\", \"decoder.transformer.0.mlp.fc1.weight\", \"decoder.transformer.0.mlp.fc1.bias\", \"decoder.transformer.0.mlp.fc2.weight\", \"decoder.transformer.0.mlp.fc2.bias\", \"decoder.transformer.1.norm1.weight\", \"decoder.transformer.1.norm1.bias\", \"decoder.transformer.1.attn.qkv.weight\", \"decoder.transformer.1.attn.proj.weight\", \"decoder.transformer.1.attn.proj.bias\", \"decoder.transformer.1.norm2.weight\", \"decoder.transformer.1.norm2.bias\", \"decoder.transformer.1.mlp.fc1.weight\", \"decoder.transformer.1.mlp.fc1.bias\", \"decoder.transformer.1.mlp.fc2.weight\", \"decoder.transformer.1.mlp.fc2.bias\", \"decoder.transformer.2.norm1.weight\", \"decoder.transformer.2.norm1.bias\", \"decoder.transformer.2.attn.qkv.weight\", \"decoder.transformer.2.attn.proj.weight\", \"decoder.transformer.2.attn.proj.bias\", \"decoder.transformer.2.norm2.weight\", \"decoder.transformer.2.norm2.bias\", \"decoder.transformer.2.mlp.fc1.weight\", \"decoder.transformer.2.mlp.fc1.bias\", \"decoder.transformer.2.mlp.fc2.weight\", \"decoder.transformer.2.mlp.fc2.bias\", \"decoder.transformer.3.norm1.weight\", \"decoder.transformer.3.norm1.bias\", \"decoder.transformer.3.attn.qkv.weight\", \"decoder.transformer.3.attn.proj.weight\", \"decoder.transformer.3.attn.proj.bias\", \"decoder.transformer.3.norm2.weight\", \"decoder.transformer.3.norm2.bias\", \"decoder.transformer.3.mlp.fc1.weight\", \"decoder.transformer.3.mlp.fc1.bias\", \"decoder.transformer.3.mlp.fc2.weight\", \"decoder.transformer.3.mlp.fc2.bias\", \"decoder.head.weight\", \"decoder.head.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.encoder.cls_token\", \"_orig_mod.encoder.pos_embedding\", \"_orig_mod.encoder.patchify.weight\", \"_orig_mod.encoder.patchify.bias\", \"_orig_mod.encoder.transformer.0.norm1.weight\", \"_orig_mod.encoder.transformer.0.norm1.bias\", \"_orig_mod.encoder.transformer.0.attn.qkv.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.bias\", \"_orig_mod.encoder.transformer.0.norm2.weight\", \"_orig_mod.encoder.transformer.0.norm2.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.1.norm1.weight\", \"_orig_mod.encoder.transformer.1.norm1.bias\", \"_orig_mod.encoder.transformer.1.attn.qkv.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.bias\", \"_orig_mod.encoder.transformer.1.norm2.weight\", \"_orig_mod.encoder.transformer.1.norm2.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.2.norm1.weight\", \"_orig_mod.encoder.transformer.2.norm1.bias\", \"_orig_mod.encoder.transformer.2.attn.qkv.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.bias\", \"_orig_mod.encoder.transformer.2.norm2.weight\", \"_orig_mod.encoder.transformer.2.norm2.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.3.norm1.weight\", \"_orig_mod.encoder.transformer.3.norm1.bias\", \"_orig_mod.encoder.transformer.3.attn.qkv.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.bias\", \"_orig_mod.encoder.transformer.3.norm2.weight\", \"_orig_mod.encoder.transformer.3.norm2.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.4.norm1.weight\", \"_orig_mod.encoder.transformer.4.norm1.bias\", \"_orig_mod.encoder.transformer.4.attn.qkv.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.bias\", \"_orig_mod.encoder.transformer.4.norm2.weight\", \"_orig_mod.encoder.transformer.4.norm2.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.5.norm1.weight\", \"_orig_mod.encoder.transformer.5.norm1.bias\", \"_orig_mod.encoder.transformer.5.attn.qkv.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.bias\", \"_orig_mod.encoder.transformer.5.norm2.weight\", \"_orig_mod.encoder.transformer.5.norm2.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.6.norm1.weight\", \"_orig_mod.encoder.transformer.6.norm1.bias\", \"_orig_mod.encoder.transformer.6.attn.qkv.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.bias\", \"_orig_mod.encoder.transformer.6.norm2.weight\", \"_orig_mod.encoder.transformer.6.norm2.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.7.norm1.weight\", \"_orig_mod.encoder.transformer.7.norm1.bias\", \"_orig_mod.encoder.transformer.7.attn.qkv.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.bias\", \"_orig_mod.encoder.transformer.7.norm2.weight\", \"_orig_mod.encoder.transformer.7.norm2.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.8.norm1.weight\", \"_orig_mod.encoder.transformer.8.norm1.bias\", \"_orig_mod.encoder.transformer.8.attn.qkv.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.bias\", \"_orig_mod.encoder.transformer.8.norm2.weight\", \"_orig_mod.encoder.transformer.8.norm2.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.9.norm1.weight\", \"_orig_mod.encoder.transformer.9.norm1.bias\", \"_orig_mod.encoder.transformer.9.attn.qkv.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.bias\", \"_orig_mod.encoder.transformer.9.norm2.weight\", \"_orig_mod.encoder.transformer.9.norm2.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.10.norm1.weight\", \"_orig_mod.encoder.transformer.10.norm1.bias\", \"_orig_mod.encoder.transformer.10.attn.qkv.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.bias\", \"_orig_mod.encoder.transformer.10.norm2.weight\", \"_orig_mod.encoder.transformer.10.norm2.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.11.norm1.weight\", \"_orig_mod.encoder.transformer.11.norm1.bias\", \"_orig_mod.encoder.transformer.11.attn.qkv.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.bias\", \"_orig_mod.encoder.transformer.11.norm2.weight\", \"_orig_mod.encoder.transformer.11.norm2.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc2.bias\", \"_orig_mod.encoder.layer_norm.weight\", \"_orig_mod.encoder.layer_norm.bias\", \"_orig_mod.decoder.mask_token\", \"_orig_mod.decoder.pos_embedding\", \"_orig_mod.decoder.transformer.0.norm1.weight\", \"_orig_mod.decoder.transformer.0.norm1.bias\", \"_orig_mod.decoder.transformer.0.attn.qkv.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.bias\", \"_orig_mod.decoder.transformer.0.norm2.weight\", \"_orig_mod.decoder.transformer.0.norm2.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.1.norm1.weight\", \"_orig_mod.decoder.transformer.1.norm1.bias\", \"_orig_mod.decoder.transformer.1.attn.qkv.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.bias\", \"_orig_mod.decoder.transformer.1.norm2.weight\", \"_orig_mod.decoder.transformer.1.norm2.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.2.norm1.weight\", \"_orig_mod.decoder.transformer.2.norm1.bias\", \"_orig_mod.decoder.transformer.2.attn.qkv.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.bias\", \"_orig_mod.decoder.transformer.2.norm2.weight\", \"_orig_mod.decoder.transformer.2.norm2.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.3.norm1.weight\", \"_orig_mod.decoder.transformer.3.norm1.bias\", \"_orig_mod.decoder.transformer.3.attn.qkv.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.bias\", \"_orig_mod.decoder.transformer.3.norm2.weight\", \"_orig_mod.decoder.transformer.3.norm2.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.decoder.head.weight\", \"_orig_mod.decoder.head.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/teamspace/studios/this_studio/real_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MAE_ViT:\n\tMissing key(s) in state_dict: \"encoder.cls_token\", \"encoder.pos_embedding\", \"encoder.patchify.weight\", \"encoder.patchify.bias\", \"encoder.transformer.0.norm1.weight\", \"encoder.transformer.0.norm1.bias\", \"encoder.transformer.0.attn.qkv.weight\", \"encoder.transformer.0.attn.proj.weight\", \"encoder.transformer.0.attn.proj.bias\", \"encoder.transformer.0.norm2.weight\", \"encoder.transformer.0.norm2.bias\", \"encoder.transformer.0.mlp.fc1.weight\", \"encoder.transformer.0.mlp.fc1.bias\", \"encoder.transformer.0.mlp.fc2.weight\", \"encoder.transformer.0.mlp.fc2.bias\", \"encoder.transformer.1.norm1.weight\", \"encoder.transformer.1.norm1.bias\", \"encoder.transformer.1.attn.qkv.weight\", \"encoder.transformer.1.attn.proj.weight\", \"encoder.transformer.1.attn.proj.bias\", \"encoder.transformer.1.norm2.weight\", \"encoder.transformer.1.norm2.bias\", \"encoder.transformer.1.mlp.fc1.weight\", \"encoder.transformer.1.mlp.fc1.bias\", \"encoder.transformer.1.mlp.fc2.weight\", \"encoder.transformer.1.mlp.fc2.bias\", \"encoder.transformer.2.norm1.weight\", \"encoder.transformer.2.norm1.bias\", \"encoder.transformer.2.attn.qkv.weight\", \"encoder.transformer.2.attn.proj.weight\", \"encoder.transformer.2.attn.proj.bias\", \"encoder.transformer.2.norm2.weight\", \"encoder.transformer.2.norm2.bias\", \"encoder.transformer.2.mlp.fc1.weight\", \"encoder.transformer.2.mlp.fc1.bias\", \"encoder.transformer.2.mlp.fc2.weight\", \"encoder.transformer.2.mlp.fc2.bias\", \"encoder.transformer.3.norm1.weight\", \"encoder.transformer.3.norm1.bias\", \"encoder.transformer.3.attn.qkv.weight\", \"encoder.transformer.3.attn.proj.weight\", \"encoder.transformer.3.attn.proj.bias\", \"encoder.transformer.3.norm2.weight\", \"encoder.transformer.3.norm2.bias\", \"encoder.transformer.3.mlp.fc1.weight\", \"encoder.transformer.3.mlp.fc1.bias\", \"encoder.transformer.3.mlp.fc2.weight\", \"encoder.transformer.3.mlp.fc2.bias\", \"encoder.transformer.4.norm1.weight\", \"encoder.transformer.4.norm1.bias\", \"encoder.transformer.4.attn.qkv.weight\", \"encoder.transformer.4.attn.proj.weight\", \"encoder.transformer.4.attn.proj.bias\", \"encoder.transformer.4.norm2.weight\", \"encoder.transformer.4.norm2.bias\", \"encoder.transformer.4.mlp.fc1.weight\", \"encoder.transformer.4.mlp.fc1.bias\", \"encoder.transformer.4.mlp.fc2.weight\", \"encoder.transformer.4.mlp.fc2.bias\", \"encoder.transformer.5.norm1.weight\", \"encoder.transformer.5.norm1.bias\", \"encoder.transformer.5.attn.qkv.weight\", \"encoder.transformer.5.attn.proj.weight\", \"encoder.transformer.5.attn.proj.bias\", \"encoder.transformer.5.norm2.weight\", \"encoder.transformer.5.norm2.bias\", \"encoder.transformer.5.mlp.fc1.weight\", \"encoder.transformer.5.mlp.fc1.bias\", \"encoder.transformer.5.mlp.fc2.weight\", \"encoder.transformer.5.mlp.fc2.bias\", \"encoder.transformer.6.norm1.weight\", \"encoder.transformer.6.norm1.bias\", \"encoder.transformer.6.attn.qkv.weight\", \"encoder.transformer.6.attn.proj.weight\", \"encoder.transformer.6.attn.proj.bias\", \"encoder.transformer.6.norm2.weight\", \"encoder.transformer.6.norm2.bias\", \"encoder.transformer.6.mlp.fc1.weight\", \"encoder.transformer.6.mlp.fc1.bias\", \"encoder.transformer.6.mlp.fc2.weight\", \"encoder.transformer.6.mlp.fc2.bias\", \"encoder.transformer.7.norm1.weight\", \"encoder.transformer.7.norm1.bias\", \"encoder.transformer.7.attn.qkv.weight\", \"encoder.transformer.7.attn.proj.weight\", \"encoder.transformer.7.attn.proj.bias\", \"encoder.transformer.7.norm2.weight\", \"encoder.transformer.7.norm2.bias\", \"encoder.transformer.7.mlp.fc1.weight\", \"encoder.transformer.7.mlp.fc1.bias\", \"encoder.transformer.7.mlp.fc2.weight\", \"encoder.transformer.7.mlp.fc2.bias\", \"encoder.transformer.8.norm1.weight\", \"encoder.transformer.8.norm1.bias\", \"encoder.transformer.8.attn.qkv.weight\", \"encoder.transformer.8.attn.proj.weight\", \"encoder.transformer.8.attn.proj.bias\", \"encoder.transformer.8.norm2.weight\", \"encoder.transformer.8.norm2.bias\", \"encoder.transformer.8.mlp.fc1.weight\", \"encoder.transformer.8.mlp.fc1.bias\", \"encoder.transformer.8.mlp.fc2.weight\", \"encoder.transformer.8.mlp.fc2.bias\", \"encoder.transformer.9.norm1.weight\", \"encoder.transformer.9.norm1.bias\", \"encoder.transformer.9.attn.qkv.weight\", \"encoder.transformer.9.attn.proj.weight\", \"encoder.transformer.9.attn.proj.bias\", \"encoder.transformer.9.norm2.weight\", \"encoder.transformer.9.norm2.bias\", \"encoder.transformer.9.mlp.fc1.weight\", \"encoder.transformer.9.mlp.fc1.bias\", \"encoder.transformer.9.mlp.fc2.weight\", \"encoder.transformer.9.mlp.fc2.bias\", \"encoder.transformer.10.norm1.weight\", \"encoder.transformer.10.norm1.bias\", \"encoder.transformer.10.attn.qkv.weight\", \"encoder.transformer.10.attn.proj.weight\", \"encoder.transformer.10.attn.proj.bias\", \"encoder.transformer.10.norm2.weight\", \"encoder.transformer.10.norm2.bias\", \"encoder.transformer.10.mlp.fc1.weight\", \"encoder.transformer.10.mlp.fc1.bias\", \"encoder.transformer.10.mlp.fc2.weight\", \"encoder.transformer.10.mlp.fc2.bias\", \"encoder.transformer.11.norm1.weight\", \"encoder.transformer.11.norm1.bias\", \"encoder.transformer.11.attn.qkv.weight\", \"encoder.transformer.11.attn.proj.weight\", \"encoder.transformer.11.attn.proj.bias\", \"encoder.transformer.11.norm2.weight\", \"encoder.transformer.11.norm2.bias\", \"encoder.transformer.11.mlp.fc1.weight\", \"encoder.transformer.11.mlp.fc1.bias\", \"encoder.transformer.11.mlp.fc2.weight\", \"encoder.transformer.11.mlp.fc2.bias\", \"encoder.layer_norm.weight\", \"encoder.layer_norm.bias\", \"decoder.mask_token\", \"decoder.pos_embedding\", \"decoder.transformer.0.norm1.weight\", \"decoder.transformer.0.norm1.bias\", \"decoder.transformer.0.attn.qkv.weight\", \"decoder.transformer.0.attn.proj.weight\", \"decoder.transformer.0.attn.proj.bias\", \"decoder.transformer.0.norm2.weight\", \"decoder.transformer.0.norm2.bias\", \"decoder.transformer.0.mlp.fc1.weight\", \"decoder.transformer.0.mlp.fc1.bias\", \"decoder.transformer.0.mlp.fc2.weight\", \"decoder.transformer.0.mlp.fc2.bias\", \"decoder.transformer.1.norm1.weight\", \"decoder.transformer.1.norm1.bias\", \"decoder.transformer.1.attn.qkv.weight\", \"decoder.transformer.1.attn.proj.weight\", \"decoder.transformer.1.attn.proj.bias\", \"decoder.transformer.1.norm2.weight\", \"decoder.transformer.1.norm2.bias\", \"decoder.transformer.1.mlp.fc1.weight\", \"decoder.transformer.1.mlp.fc1.bias\", \"decoder.transformer.1.mlp.fc2.weight\", \"decoder.transformer.1.mlp.fc2.bias\", \"decoder.transformer.2.norm1.weight\", \"decoder.transformer.2.norm1.bias\", \"decoder.transformer.2.attn.qkv.weight\", \"decoder.transformer.2.attn.proj.weight\", \"decoder.transformer.2.attn.proj.bias\", \"decoder.transformer.2.norm2.weight\", \"decoder.transformer.2.norm2.bias\", \"decoder.transformer.2.mlp.fc1.weight\", \"decoder.transformer.2.mlp.fc1.bias\", \"decoder.transformer.2.mlp.fc2.weight\", \"decoder.transformer.2.mlp.fc2.bias\", \"decoder.transformer.3.norm1.weight\", \"decoder.transformer.3.norm1.bias\", \"decoder.transformer.3.attn.qkv.weight\", \"decoder.transformer.3.attn.proj.weight\", \"decoder.transformer.3.attn.proj.bias\", \"decoder.transformer.3.norm2.weight\", \"decoder.transformer.3.norm2.bias\", \"decoder.transformer.3.mlp.fc1.weight\", \"decoder.transformer.3.mlp.fc1.bias\", \"decoder.transformer.3.mlp.fc2.weight\", \"decoder.transformer.3.mlp.fc2.bias\", \"decoder.head.weight\", \"decoder.head.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.encoder.cls_token\", \"_orig_mod.encoder.pos_embedding\", \"_orig_mod.encoder.patchify.weight\", \"_orig_mod.encoder.patchify.bias\", \"_orig_mod.encoder.transformer.0.norm1.weight\", \"_orig_mod.encoder.transformer.0.norm1.bias\", \"_orig_mod.encoder.transformer.0.attn.qkv.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.weight\", \"_orig_mod.encoder.transformer.0.attn.proj.bias\", \"_orig_mod.encoder.transformer.0.norm2.weight\", \"_orig_mod.encoder.transformer.0.norm2.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.1.norm1.weight\", \"_orig_mod.encoder.transformer.1.norm1.bias\", \"_orig_mod.encoder.transformer.1.attn.qkv.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.weight\", \"_orig_mod.encoder.transformer.1.attn.proj.bias\", \"_orig_mod.encoder.transformer.1.norm2.weight\", \"_orig_mod.encoder.transformer.1.norm2.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.2.norm1.weight\", \"_orig_mod.encoder.transformer.2.norm1.bias\", \"_orig_mod.encoder.transformer.2.attn.qkv.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.weight\", \"_orig_mod.encoder.transformer.2.attn.proj.bias\", \"_orig_mod.encoder.transformer.2.norm2.weight\", \"_orig_mod.encoder.transformer.2.norm2.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.3.norm1.weight\", \"_orig_mod.encoder.transformer.3.norm1.bias\", \"_orig_mod.encoder.transformer.3.attn.qkv.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.weight\", \"_orig_mod.encoder.transformer.3.attn.proj.bias\", \"_orig_mod.encoder.transformer.3.norm2.weight\", \"_orig_mod.encoder.transformer.3.norm2.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.4.norm1.weight\", \"_orig_mod.encoder.transformer.4.norm1.bias\", \"_orig_mod.encoder.transformer.4.attn.qkv.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.weight\", \"_orig_mod.encoder.transformer.4.attn.proj.bias\", \"_orig_mod.encoder.transformer.4.norm2.weight\", \"_orig_mod.encoder.transformer.4.norm2.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.4.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.4.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.5.norm1.weight\", \"_orig_mod.encoder.transformer.5.norm1.bias\", \"_orig_mod.encoder.transformer.5.attn.qkv.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.weight\", \"_orig_mod.encoder.transformer.5.attn.proj.bias\", \"_orig_mod.encoder.transformer.5.norm2.weight\", \"_orig_mod.encoder.transformer.5.norm2.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.5.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.5.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.6.norm1.weight\", \"_orig_mod.encoder.transformer.6.norm1.bias\", \"_orig_mod.encoder.transformer.6.attn.qkv.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.weight\", \"_orig_mod.encoder.transformer.6.attn.proj.bias\", \"_orig_mod.encoder.transformer.6.norm2.weight\", \"_orig_mod.encoder.transformer.6.norm2.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.6.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.6.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.7.norm1.weight\", \"_orig_mod.encoder.transformer.7.norm1.bias\", \"_orig_mod.encoder.transformer.7.attn.qkv.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.weight\", \"_orig_mod.encoder.transformer.7.attn.proj.bias\", \"_orig_mod.encoder.transformer.7.norm2.weight\", \"_orig_mod.encoder.transformer.7.norm2.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.7.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.7.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.8.norm1.weight\", \"_orig_mod.encoder.transformer.8.norm1.bias\", \"_orig_mod.encoder.transformer.8.attn.qkv.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.weight\", \"_orig_mod.encoder.transformer.8.attn.proj.bias\", \"_orig_mod.encoder.transformer.8.norm2.weight\", \"_orig_mod.encoder.transformer.8.norm2.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.8.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.8.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.9.norm1.weight\", \"_orig_mod.encoder.transformer.9.norm1.bias\", \"_orig_mod.encoder.transformer.9.attn.qkv.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.weight\", \"_orig_mod.encoder.transformer.9.attn.proj.bias\", \"_orig_mod.encoder.transformer.9.norm2.weight\", \"_orig_mod.encoder.transformer.9.norm2.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.9.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.9.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.10.norm1.weight\", \"_orig_mod.encoder.transformer.10.norm1.bias\", \"_orig_mod.encoder.transformer.10.attn.qkv.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.weight\", \"_orig_mod.encoder.transformer.10.attn.proj.bias\", \"_orig_mod.encoder.transformer.10.norm2.weight\", \"_orig_mod.encoder.transformer.10.norm2.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.10.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.10.mlp.fc2.bias\", \"_orig_mod.encoder.transformer.11.norm1.weight\", \"_orig_mod.encoder.transformer.11.norm1.bias\", \"_orig_mod.encoder.transformer.11.attn.qkv.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.weight\", \"_orig_mod.encoder.transformer.11.attn.proj.bias\", \"_orig_mod.encoder.transformer.11.norm2.weight\", \"_orig_mod.encoder.transformer.11.norm2.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc1.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc1.bias\", \"_orig_mod.encoder.transformer.11.mlp.fc2.weight\", \"_orig_mod.encoder.transformer.11.mlp.fc2.bias\", \"_orig_mod.encoder.layer_norm.weight\", \"_orig_mod.encoder.layer_norm.bias\", \"_orig_mod.decoder.mask_token\", \"_orig_mod.decoder.pos_embedding\", \"_orig_mod.decoder.transformer.0.norm1.weight\", \"_orig_mod.decoder.transformer.0.norm1.bias\", \"_orig_mod.decoder.transformer.0.attn.qkv.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.weight\", \"_orig_mod.decoder.transformer.0.attn.proj.bias\", \"_orig_mod.decoder.transformer.0.norm2.weight\", \"_orig_mod.decoder.transformer.0.norm2.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.0.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.0.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.1.norm1.weight\", \"_orig_mod.decoder.transformer.1.norm1.bias\", \"_orig_mod.decoder.transformer.1.attn.qkv.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.weight\", \"_orig_mod.decoder.transformer.1.attn.proj.bias\", \"_orig_mod.decoder.transformer.1.norm2.weight\", \"_orig_mod.decoder.transformer.1.norm2.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.1.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.1.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.2.norm1.weight\", \"_orig_mod.decoder.transformer.2.norm1.bias\", \"_orig_mod.decoder.transformer.2.attn.qkv.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.weight\", \"_orig_mod.decoder.transformer.2.attn.proj.bias\", \"_orig_mod.decoder.transformer.2.norm2.weight\", \"_orig_mod.decoder.transformer.2.norm2.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.2.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.2.mlp.fc2.bias\", \"_orig_mod.decoder.transformer.3.norm1.weight\", \"_orig_mod.decoder.transformer.3.norm1.bias\", \"_orig_mod.decoder.transformer.3.attn.qkv.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.weight\", \"_orig_mod.decoder.transformer.3.attn.proj.bias\", \"_orig_mod.decoder.transformer.3.norm2.weight\", \"_orig_mod.decoder.transformer.3.norm2.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc1.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc1.bias\", \"_orig_mod.decoder.transformer.3.mlp.fc2.weight\", \"_orig_mod.decoder.transformer.3.mlp.fc2.bias\", \"_orig_mod.decoder.head.weight\", \"_orig_mod.decoder.head.bias\". "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/teamspace/studios/this_studio/real_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'\n",
    "image = Image.open(image_path)\n",
    "# Ensure you have the correct model initialized\n",
    "# model = MAE_ViT(...)  # Use your actual model initialization here\n",
    "run_inference(image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "\n",
    "img = Image.open(\"/teamspace/studios/this_studio/orange.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.resize((32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the transformation to preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize the image to 32x32\n",
    "    transforms.ToTensor(),        # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'  # Replace with your image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "image = image.to(DEVICE)  # Move to the appropriate device\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, mask, ids_restore = model(image)\n",
    "\n",
    "# Reshape the mask to match the spatial dimensions of the image\n",
    "patch_size = 4\n",
    "num_patches_per_dim = 32 // patch_size  # 8 patches per dimension\n",
    "\n",
    "# Reshape the mask to [1, 8, 8] and then expand to [1, 1, 32, 32]\n",
    "mask_reshaped = mask.view(1, num_patches_per_dim, num_patches_per_dim)\n",
    "mask_upsampled = mask_reshaped.unsqueeze(1).repeat(1, 3, patch_size, patch_size)\n",
    "\n",
    "# Apply the mask to the original image\n",
    "masked_image = image * (1 - mask_upsampled)\n",
    "\n",
    "# Apply the mask to the original image\n",
    "masked_image = image * (1 - mask_upsampled)\n",
    "\n",
    "out_reshaped = out.view(1, 3, 32, 32)\n",
    "# Concatenate the original, masked, and reconstructed images\n",
    "img = torch.cat([masked_image, out_reshaped, image], dim=0)\n",
    "\n",
    "# Convert tensors to numpy arrays for visualization\n",
    "def tensor_to_image(tensor):\n",
    "    return tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "original_image_np = tensor_to_image(image)\n",
    "masked_image_np = tensor_to_image(masked_image)\n",
    "reconstructed_image_np = tensor_to_image(out_reshaped)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Masked Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(masked_image_np)\n",
    "plt.title('Masked Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Reconstructed Image\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(reconstructed_image_np)\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(original_image_np)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_image torch.Size([1, 3, 32, 32])\n",
    "out torch.Size([1, 64, 48])\n",
    "image torch.Size([1, 3, 32, 32])\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "Cell In[82], line 42\n",
    "     40 print(\"image\",image.size())\n",
    "     41 # Concatenate the original, masked, and reconstructed images\n",
    "---> 42 img = torch.cat([masked_image, out, image], dim=0)\n",
    "     44 # Convert tensors to numpy arrays for visualization\n",
    "     45 def tensor_to_image(tensor):\n",
    "\n",
    "RuntimeError: Tensors must have same number of dimensions: got 4 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the transformation to preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize the image to 32x32\n",
    "    transforms.ToTensor(),        # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'  # Replace with your image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "image = image.to(DEVICE)  # Move to the appropriate device\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, mask, ids_restore = model(image)\n",
    "\n",
    "# Assuming patch_size is 4 (as per your earlier code)\n",
    "patch_size = 4\n",
    "num_patches_per_dim = 32 // patch_size  # 32x32 image with 4x4 patches\n",
    "\n",
    "# Reshape the output to reconstruct the image\n",
    "batch_size, num_patches, patch_dim = out.shape\n",
    "channels = 3  # Assuming RGB images\n",
    "reconstructed_image = out.view(batch_size, num_patches_per_dim, num_patches_per_dim, channels, patch_size, patch_size)\n",
    "reconstructed_image = reconstructed_image.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "reconstructed_image = reconstructed_image.view(batch_size, channels, 32, 32)\n",
    "\n",
    "# Reshape the mask to match the patch layout\n",
    "mask_np = mask.view(num_patches_per_dim, num_patches_per_dim).cpu().numpy()\n",
    "\n",
    "# Function to visualize the image and mask\n",
    "def visualize_masking(original_image, mask, reconstructed_image):\n",
    "    # Convert tensors to numpy arrays for visualization\n",
    "    original_image_np = original_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    reconstructed_image_np = reconstructed_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Plot the images\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_image_np)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Reconstructed Image\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(reconstructed_image_np)\n",
    "    plt.title('Reconstructed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_masking(image, mask_np, reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Get the output and mask from the model\n",
    "out, mask, _ = model(img)\n",
    "\n",
    "# Reshape the mask to match the image dimensions\n",
    "patch_size = 4\n",
    "num_patches = (32 // patch_size) ** 2\n",
    "mask = mask.view(1, num_patches, 1)\n",
    "mask = mask.permute(0, 2, 1).contiguous()\n",
    "mask = mask.view(1, 1, 8, 8)\n",
    "\n",
    "# Upsample the mask to match the image dimensions\n",
    "mask = torch.nn.functional.interpolate(mask, size=(32, 32), mode='nearest')\n",
    "\n",
    "# Convert the mask to a numpy array\n",
    "mask = mask.cpu().numpy().squeeze()\n",
    "\n",
    "# Plot the original image and the masked image\n",
    "img = img.cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img * mask[:, :, np.newaxis])\n",
    "plt.title('Masked Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model.eval()\n",
    "\n",
    "# Load the image\n",
    "img_path = '/teamspace/studios/this_studio/orange.jpg'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Get the output and mask from the model\n",
    "out, mask, _ = model(img)\n",
    "\n",
    "# Reshape the output to match the image dimensions\n",
    "patch_size = 4\n",
    "num_patches = (32 // patch_size) ** 2\n",
    "out = out.view(1, num_patches, -1)\n",
    "\n",
    "# Convert the output to a numpy array\n",
    "out = out.cpu().detach().numpy().squeeze().transpose(1, 0)\n",
    "out = out.reshape(8, 8, 3)\n",
    "\n",
    "# Upsample the output to match the image dimensions\n",
    "out = np.repeat(out, patch_size, axis=0)\n",
    "out = np.repeat(out, patch_size, axis=1)\n",
    "\n",
    "# Convert the mask to a numpy array\n",
    "mask = mask.cpu().numpy().squeeze()\n",
    "\n",
    "# Plot the original image, the masked image, and the predicted image\n",
    "img = img.cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img * mask[:, :, np.newaxis])\n",
    "plt.title('Masked Image')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(out.clip(0, 1))\n",
    "plt.title('Predicted Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model.eval()\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Mean and std for each of the 3 channels\n",
    "])\n",
    "\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    out, mask , ids_restore = model(img)\n",
    "\n",
    "# Reshape the output to match the patch structure\n",
    "patch_size = 4  # 224 / 14 = 16\n",
    "num_patches = (32 // patch_size) ** 2  # 14 * 14 = 196\n",
    "image_patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "image_patches = image_patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "image_patches = image_patches.view(img.size(0), num_patches, -1)  # [1, 196, 768]\n",
    "\n",
    "# Mask the image patches\n",
    "masked_img = image_patches * mask.unsqueeze(-1)\n",
    "\n",
    "print(\"Shape of masked_img before decoder:\", masked_img.shape)  # Debugging shape of input\n",
    "print(\"Shape of ids_restore:\", ids_restore.shape)  # Check ids_restore shape\n",
    "\n",
    "# Use the decoder to predict the original image\n",
    "predicted_img = model.decoder(masked_img, ids_restore)\n",
    "\n",
    "# Reshape the predicted image to match the original image shape\n",
    "predicted_img = predicted_img.view(1, 14, 14, 3, 16, 16)\n",
    "predicted_img = predicted_img.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "predicted_img = predicted_img.view(1, 3, 224, 224)\n",
    "\n",
    "# Convert the predicted image to a numpy array\n",
    "predicted_img = predicted_img.squeeze(0).cpu().numpy()\n",
    "predicted_img = np.transpose(predicted_img, (1, 2, 0))\n",
    "predicted_img = predicted_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "\n",
    "# Display the original, masked, and predicted images\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.transpose(img.squeeze(0).cpu().numpy(), (1, 2, 0)) * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 3, 2)\n",
    "masked_img = masked_img.view(1, 14, 14, 3, 16, 16)\n",
    "masked_img = masked_img.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "masked_img = masked_img.view(1, 3, 224, 224)\n",
    "masked_img = masked_img.squeeze(0).cpu().numpy()\n",
    "masked_img = np.transpose(masked_img, (1, 2, 0))\n",
    "masked_img = masked_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "plt.imshow(masked_img)\n",
    "plt.title('Masked Image')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(predicted_img)\n",
    "plt.title('Predicted Image')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the utils\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def run_one_image(img, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the image\n",
    "    x = torch.tensor(img).float().to(device)\n",
    "    x = x.unsqueeze(dim=0)\n",
    "    x = torch.einsum('nhwc->nchw', x)\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        out, mask = model(x)\n",
    "\n",
    "    # Reshape the output\n",
    "    patch_size = 16\n",
    "    num_patches = (224 // patch_size) ** 2\n",
    "    out = out.view(1, 14, 14, 3, patch_size, patch_size)\n",
    "    out = out.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "    out = out.view(1, 3, 224, 224)\n",
    "\n",
    "    # Reshape the mask\n",
    "    mask = mask.view(1, 14, 14, 1).repeat(1, 1, 1, patch_size**2 * 3)\n",
    "    mask = mask.view(1, 3, 224, 224)\n",
    "\n",
    "    # Convert tensors back to image format\n",
    "    x = torch.einsum('nchw->nhwc', x)\n",
    "    out = torch.einsum('nchw->nhwc', out)\n",
    "    mask = torch.einsum('nchw->nhwc', mask)\n",
    "\n",
    "    # Create masked and reconstructed images\n",
    "    im_masked = x * (1 - mask)\n",
    "    im_paste = x * (1 - mask) + out * mask\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(24, 6))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0].cpu(), \"Original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0].cpu(), \"Masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(out[0].cpu(), \"Reconstruction\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show_image(im_paste[0].cpu(), \"Reconstruction + Visible\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "\n",
    "img = transform(img).permute(1, 2, 0).numpy()\n",
    "\n",
    "# Run the visualization\n",
    "run_one_image(img, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify(x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = 16\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
