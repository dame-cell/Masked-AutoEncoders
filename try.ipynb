{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from utils import setup_seed ,  count_parameters  , loading_data ,  ImageDataset\n",
    "from modeling_mae import MAE_ViT \n",
    "from  torch.utils.data import DataLoader \n",
    "from tqdm.auto import tqdm \n",
    "import time \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch.nn.functional as F\n",
    "from configuration import MAEConfig\n",
    "\n",
    "config = MAEConfig()\n",
    "\n",
    "from datasets import load_dataset \n",
    "import torchvision\n",
    "\n",
    "setup_seed(seed=42)\n",
    "\n",
    "COMPILE = False \n",
    "EPOCHS = 100\n",
    "LR = 5e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EVAL_INTERVAL  = 100\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "\n",
    "# Define the transform with resizing to 224x224\n",
    "transform = Compose([\n",
    "    Resize((32, 32)),  # Resizes images to 224x224\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # Normalize with mean and std for 3 channels (RGB)\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset with resizing\n",
    "train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ImageDataset(\u001b[43mtrain_data\u001b[49m)\n\u001b[1;32m      2\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ImageDataset(test_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageDataset(train_data)\n",
    "val_dataset = ImageDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader=  DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size =128, \n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 7.21M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAE_ViT(\n",
       "  (encoder): MAE_Encoder(\n",
       "    (shuffle): PatchShuffle()\n",
       "    (patchify): Conv2d(3, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (transformer): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): MAE_Decoder(\n",
       "    (transformer): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (head): Linear(in_features=192, out_features=12, bias=True)\n",
       "    (patch2img): Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=2, p2=2, h=16)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MAE_ViT(config=config)\n",
    "print(\"Model Parameters:\",count_parameters(model))\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = 16\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "def unpatchify(x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = 16\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x2400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# ImageNet normalization constants\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def run_inference(image, model):\n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # Resize to the model's input size\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std)  # Normalize\n",
    "    ])\n",
    "\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    image = image.convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    model.to(\"cpu\")\n",
    "    image = image.to(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "        out, mask = model(image)\n",
    "\n",
    "        \n",
    "        # Both out and mask are already in the correct shape [1, 3, 32, 32]\n",
    "        y = out.detach().cpu()\n",
    "        mask = mask.detach().cpu()\n",
    "\n",
    "        # Convert to NHWC format for visualization\n",
    "        x = image.permute(0, 2, 3, 1).cpu()  # Original image\n",
    "        y = y.permute(0, 2, 3, 1)  # Reconstruction\n",
    "        mask = mask.permute(0, 2, 3, 1)  # Mask\n",
    "\n",
    "        # Masked image\n",
    "        im_masked = x * (1 - mask)\n",
    "\n",
    "        # MAE reconstruction pasted with visible patches\n",
    "        im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "        # Plot results\n",
    "        plt.rcParams['figure.figsize'] = [24, 24]\n",
    "\n",
    "        plt.subplot(1, 4, 1)\n",
    "        show_image(x[0], \"Original\")\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        show_image(im_masked[0], \"Masked\")\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        show_image(y[0], \"Reconstruction\")\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        show_image(im_paste[0], \"Reconstruction + Visible\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'\n",
    "image = Image.open(image_path)\n",
    "# Ensure you have the correct model initialized\n",
    "# model = MAE_ViT(...)  # Use your actual model initialization here\n",
    "run_inference(image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "Epoch 1/120: 100%|| 391/391 [03:34<00:00,  1.82it/s, loss=0.1691, step_time=43784.2903ms, lr=0.000010]\n",
      "Validation 1/120: 100%|| 79/79 [00:47<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 - Train loss: 0.1837, Val loss: 0.1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/120: 100%|| 391/391 [01:53<00:00,  3.46it/s, loss=0.1496, step_time=183.8424ms, lr=0.000020]\n",
      "Validation 2/120: 100%|| 79/79 [00:06<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/120 - Train loss: 0.1620, Val loss: 0.1535\n",
      "Visualizing images after Epoch 2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 120\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "WEIGHT_DECAY= 1e-4\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_func = lambda epoch: min((epoch + 1) / (10 + 1e-8), 0.5 * (math.cos(epoch / EPOCHS * math.pi) + 1))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_func, verbose=True)\n",
    "\n",
    "print(\"Starting to train\")\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "step_times = []\n",
    "grad_norms = []\n",
    "\n",
    "model = torch.compile(model)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "def calculate_loss(preds , image , mask,mask_ratio):\n",
    "    return torch.mean((preds - image) ** 2 * mask) / mask_ratio\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_train_losses = []\n",
    "    step_times = []\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for step, (image, _) in progress_bar:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.to(DEVICE)\n",
    "\n",
    "        image = image.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out, mask = model(image)\n",
    "        loss = calculate_loss(preds=out, image=image, mask=mask, mask_ratio=0.75)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_losses.append(loss.item())\n",
    "        step_times.append((time.time() - start_time) * 1000)  # Step time in milliseconds\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'step_time': f\"{(time.time() - start_time) * 1000:.4F}ms\",\n",
    "            'lr': f\"{lr_scheduler.get_last_lr()[0]:.6f}\"\n",
    "        })\n",
    "\n",
    "    avg_train_loss = sum(epoch_train_losses) / len(epoch_train_losses)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    epoch_val_losses = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Validation {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        for val_step, (val_image, _) in val_progress_bar:\n",
    "            val_image = val_image.to(DEVICE)\n",
    "            \n",
    "            out, mask = model(val_image)\n",
    "            val_loss = calculate_loss(preds=out, image=val_image, mask=mask, mask_ratio=0.75)\n",
    "            epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = sum(epoch_val_losses) / len(epoch_val_losses)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Train loss: {avg_train_loss:.4f}, Val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Inference visualization every 10 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"Visualizing images after Epoch {epoch + 1}\")\n",
    "        model.eval()\n",
    "        model.to(\"cpu\")\n",
    "        image_path = '/teamspace/studios/this_studio/orange.jpg'\n",
    "        img = Image.open(image_path)\n",
    "        run_inference(img, model)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, \"new_model_part_3.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Visualizing images after Epoch {epoch + 1}\")\n",
    "model.eval()\n",
    "image_path = '/teamspace/studios/this_studio/fingers.jpg'\n",
    "img = Image.open(image_path)\n",
    "run_inference(img, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Validation Loss')\n",
    "plt.xlabel('Evaluation Interval')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Losses Over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Evaluation Interval')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Losses Over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define paths\n",
    "model_path = 'new_model_2.pth'\n",
    "optimizer_path = 'optimizer.pth'\n",
    "scheduler_path = 'scheduler.pth'\n",
    "\n",
    "# Save model, optimizer, and scheduler\n",
    "def save_checkpoint(model, optimizer, scheduler, model_path, optimizer_path, scheduler_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, model_path)\n",
    "\n",
    "# Example usage\n",
    "save_checkpoint(model, optimizer, scheduler, model_path, optimizer_path, scheduler_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define paths\n",
    "model_path = '/teamspace/studios/this_studio/new_model_2.pth'\n",
    "\n",
    "# Load model, optimizer, and scheduler\n",
    "def load_checkpoint(model, optimizer, scheduler, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "   \n",
    "    return model\n",
    "\n",
    "model  = load_checkpoint(model, optimizer, scheduler, model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "\n",
    "img = Image.open(\"/teamspace/studios/this_studio/orange.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.resize((32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the transformation to preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize the image to 32x32\n",
    "    transforms.ToTensor(),        # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'  # Replace with your image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "image = image.to(DEVICE)  # Move to the appropriate device\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, mask, ids_restore = model(image)\n",
    "\n",
    "# Reshape the mask to match the spatial dimensions of the image\n",
    "patch_size = 4\n",
    "num_patches_per_dim = 32 // patch_size  # 8 patches per dimension\n",
    "\n",
    "# Reshape the mask to [1, 8, 8] and then expand to [1, 1, 32, 32]\n",
    "mask_reshaped = mask.view(1, num_patches_per_dim, num_patches_per_dim)\n",
    "mask_upsampled = mask_reshaped.unsqueeze(1).repeat(1, 3, patch_size, patch_size)\n",
    "\n",
    "# Apply the mask to the original image\n",
    "masked_image = image * (1 - mask_upsampled)\n",
    "\n",
    "# Apply the mask to the original image\n",
    "masked_image = image * (1 - mask_upsampled)\n",
    "\n",
    "out_reshaped = out.view(1, 3, 32, 32)\n",
    "# Concatenate the original, masked, and reconstructed images\n",
    "img = torch.cat([masked_image, out_reshaped, image], dim=0)\n",
    "\n",
    "# Convert tensors to numpy arrays for visualization\n",
    "def tensor_to_image(tensor):\n",
    "    return tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "original_image_np = tensor_to_image(image)\n",
    "masked_image_np = tensor_to_image(masked_image)\n",
    "reconstructed_image_np = tensor_to_image(out_reshaped)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Masked Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(masked_image_np)\n",
    "plt.title('Masked Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Reconstructed Image\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(reconstructed_image_np)\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(original_image_np)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_image torch.Size([1, 3, 32, 32])\n",
    "out torch.Size([1, 64, 48])\n",
    "image torch.Size([1, 3, 32, 32])\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "Cell In[82], line 42\n",
    "     40 print(\"image\",image.size())\n",
    "     41 # Concatenate the original, masked, and reconstructed images\n",
    "---> 42 img = torch.cat([masked_image, out, image], dim=0)\n",
    "     44 # Convert tensors to numpy arrays for visualization\n",
    "     45 def tensor_to_image(tensor):\n",
    "\n",
    "RuntimeError: Tensors must have same number of dimensions: got 4 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the transformation to preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize the image to 32x32\n",
    "    transforms.ToTensor(),        # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '/teamspace/studios/this_studio/orange.jpg'  # Replace with your image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "image = image.to(DEVICE)  # Move to the appropriate device\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, mask, ids_restore = model(image)\n",
    "\n",
    "# Assuming patch_size is 4 (as per your earlier code)\n",
    "patch_size = 4\n",
    "num_patches_per_dim = 32 // patch_size  # 32x32 image with 4x4 patches\n",
    "\n",
    "# Reshape the output to reconstruct the image\n",
    "batch_size, num_patches, patch_dim = out.shape\n",
    "channels = 3  # Assuming RGB images\n",
    "reconstructed_image = out.view(batch_size, num_patches_per_dim, num_patches_per_dim, channels, patch_size, patch_size)\n",
    "reconstructed_image = reconstructed_image.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "reconstructed_image = reconstructed_image.view(batch_size, channels, 32, 32)\n",
    "\n",
    "# Reshape the mask to match the patch layout\n",
    "mask_np = mask.view(num_patches_per_dim, num_patches_per_dim).cpu().numpy()\n",
    "\n",
    "# Function to visualize the image and mask\n",
    "def visualize_masking(original_image, mask, reconstructed_image):\n",
    "    # Convert tensors to numpy arrays for visualization\n",
    "    original_image_np = original_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    reconstructed_image_np = reconstructed_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Plot the images\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_image_np)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Reconstructed Image\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(reconstructed_image_np)\n",
    "    plt.title('Reconstructed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_masking(image, mask_np, reconstructed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Get the output and mask from the model\n",
    "out, mask, _ = model(img)\n",
    "\n",
    "# Reshape the mask to match the image dimensions\n",
    "patch_size = 4\n",
    "num_patches = (32 // patch_size) ** 2\n",
    "mask = mask.view(1, num_patches, 1)\n",
    "mask = mask.permute(0, 2, 1).contiguous()\n",
    "mask = mask.view(1, 1, 8, 8)\n",
    "\n",
    "# Upsample the mask to match the image dimensions\n",
    "mask = torch.nn.functional.interpolate(mask, size=(32, 32), mode='nearest')\n",
    "\n",
    "# Convert the mask to a numpy array\n",
    "mask = mask.cpu().numpy().squeeze()\n",
    "\n",
    "# Plot the original image and the masked image\n",
    "img = img.cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img * mask[:, :, np.newaxis])\n",
    "plt.title('Masked Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model.eval()\n",
    "\n",
    "# Load the image\n",
    "img_path = '/teamspace/studios/this_studio/orange.jpg'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Get the output and mask from the model\n",
    "out, mask, _ = model(img)\n",
    "\n",
    "# Reshape the output to match the image dimensions\n",
    "patch_size = 4\n",
    "num_patches = (32 // patch_size) ** 2\n",
    "out = out.view(1, num_patches, -1)\n",
    "\n",
    "# Convert the output to a numpy array\n",
    "out = out.cpu().detach().numpy().squeeze().transpose(1, 0)\n",
    "out = out.reshape(8, 8, 3)\n",
    "\n",
    "# Upsample the output to match the image dimensions\n",
    "out = np.repeat(out, patch_size, axis=0)\n",
    "out = np.repeat(out, patch_size, axis=1)\n",
    "\n",
    "# Convert the mask to a numpy array\n",
    "mask = mask.cpu().numpy().squeeze()\n",
    "\n",
    "# Plot the original image, the masked image, and the predicted image\n",
    "img = img.cpu().numpy().squeeze().transpose(1, 2, 0)\n",
    "img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img * mask[:, :, np.newaxis])\n",
    "plt.title('Masked Image')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(out.clip(0, 1))\n",
    "plt.title('Predicted Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model.eval()\n",
    "# Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Mean and std for each of the 3 channels\n",
    "])\n",
    "\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    out, mask , ids_restore = model(img)\n",
    "\n",
    "# Reshape the output to match the patch structure\n",
    "patch_size = 4  # 224 / 14 = 16\n",
    "num_patches = (32 // patch_size) ** 2  # 14 * 14 = 196\n",
    "image_patches = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "image_patches = image_patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "image_patches = image_patches.view(img.size(0), num_patches, -1)  # [1, 196, 768]\n",
    "\n",
    "# Mask the image patches\n",
    "masked_img = image_patches * mask.unsqueeze(-1)\n",
    "\n",
    "print(\"Shape of masked_img before decoder:\", masked_img.shape)  # Debugging shape of input\n",
    "print(\"Shape of ids_restore:\", ids_restore.shape)  # Check ids_restore shape\n",
    "\n",
    "# Use the decoder to predict the original image\n",
    "predicted_img = model.decoder(masked_img, ids_restore)\n",
    "\n",
    "# Reshape the predicted image to match the original image shape\n",
    "predicted_img = predicted_img.view(1, 14, 14, 3, 16, 16)\n",
    "predicted_img = predicted_img.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "predicted_img = predicted_img.view(1, 3, 224, 224)\n",
    "\n",
    "# Convert the predicted image to a numpy array\n",
    "predicted_img = predicted_img.squeeze(0).cpu().numpy()\n",
    "predicted_img = np.transpose(predicted_img, (1, 2, 0))\n",
    "predicted_img = predicted_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "\n",
    "# Display the original, masked, and predicted images\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(np.transpose(img.squeeze(0).cpu().numpy(), (1, 2, 0)) * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 3, 2)\n",
    "masked_img = masked_img.view(1, 14, 14, 3, 16, 16)\n",
    "masked_img = masked_img.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "masked_img = masked_img.view(1, 3, 224, 224)\n",
    "masked_img = masked_img.squeeze(0).cpu().numpy()\n",
    "masked_img = np.transpose(masked_img, (1, 2, 0))\n",
    "masked_img = masked_img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "plt.imshow(masked_img)\n",
    "plt.title('Masked Image')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(predicted_img)\n",
    "plt.title('Predicted Image')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the utils\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def run_one_image(img, model, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the image\n",
    "    x = torch.tensor(img).float().to(device)\n",
    "    x = x.unsqueeze(dim=0)\n",
    "    x = torch.einsum('nhwc->nchw', x)\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        out, mask = model(x)\n",
    "\n",
    "    # Reshape the output\n",
    "    patch_size = 16\n",
    "    num_patches = (224 // patch_size) ** 2\n",
    "    out = out.view(1, 14, 14, 3, patch_size, patch_size)\n",
    "    out = out.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "    out = out.view(1, 3, 224, 224)\n",
    "\n",
    "    # Reshape the mask\n",
    "    mask = mask.view(1, 14, 14, 1).repeat(1, 1, 1, patch_size**2 * 3)\n",
    "    mask = mask.view(1, 3, 224, 224)\n",
    "\n",
    "    # Convert tensors back to image format\n",
    "    x = torch.einsum('nchw->nhwc', x)\n",
    "    out = torch.einsum('nchw->nhwc', out)\n",
    "    mask = torch.einsum('nchw->nhwc', mask)\n",
    "\n",
    "    # Create masked and reconstructed images\n",
    "    im_masked = x * (1 - mask)\n",
    "    im_paste = x * (1 - mask) + out * mask\n",
    "\n",
    "    # Visualize\n",
    "    plt.figure(figsize=(24, 6))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0].cpu(), \"Original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0].cpu(), \"Masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(out[0].cpu(), \"Reconstruction\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show_image(im_paste[0].cpu(), \"Reconstruction + Visible\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "\n",
    "img = transform(img).permute(1, 2, 0).numpy()\n",
    "\n",
    "# Run the visualization\n",
    "run_one_image(img, model, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, title=''):\n",
    "    # image is [H, W, 3]\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify(x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = 16\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
